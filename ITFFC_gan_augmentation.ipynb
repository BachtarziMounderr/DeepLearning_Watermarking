{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7a43e17",
   "metadata": {},
   "source": [
    "\n",
    "# ITFFC — Task 2: Conditional GAN (ACGAN) + Synthetic Augmentation\n",
    "\n",
    "This notebook trains a **conditional GAN (ACGAN)** on your **medical** processed images only and generates synthetic data to **balance class imbalance**.\n",
    "\n",
    "- **Input root (processed):** `C:\\Users\\bacht\\Desktop\\Master2_S1\\ITFFC\\Dataset\\medical`\n",
    "- It automatically discovers every `*_processed` leaf folder and treats each as a class.\n",
    "- Trains on GPU (CUDA) if available (e.g., RTX 3060), with **AMP mixed precision** for speed.\n",
    "- After training (**200 epochs** by default), it generates samples **per class** to reach the count of the largest class.\n",
    "- Synthetic images are saved under:  \n",
    "  `...\\Dataset\\medical_synthetic\\<class>\\*.png`\n",
    "\n",
    "> Model: **ACGAN** (Discriminator outputs real/fake + class logits; Generator conditioned on class labels).  \n",
    "> Image size default: **256×256**, channels: **3**, latent dim: **128**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16b863d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =======================================\n",
    "# Configuration\n",
    "# =======================================\n",
    "from pathlib import Path\n",
    "\n",
    "DATASET_MEDICAL_ROOT = r\"C:\\Users\\bacht\\Desktop\\Master2_S1\\ITFFC\\Dataset\\medical\"\n",
    "\n",
    "image_size   = 256\n",
    "channels     = 3\n",
    "latent_dim   = 128\n",
    "batch_size   = 32\n",
    "num_epochs   = 200\n",
    "lr_G         = 2e-4\n",
    "lr_D         = 2e-4\n",
    "beta1, beta2 = 0.5, 0.999\n",
    "num_workers  = 4\n",
    "\n",
    "samples_per_grid = 16\n",
    "save_preview_every = 5\n",
    "balance_to_largest = True\n",
    "\n",
    "OUTPUT_ROOT = Path(DATASET_MEDICAL_ROOT).parent / \"medical_synthetic\"\n",
    "CHECKPOINTS = Path(DATASET_MEDICAL_ROOT).parent / \"gan_ckpts\"\n",
    "PREVIEWS    = Path(DATASET_MEDICAL_ROOT).parent / \"gan_previews\"\n",
    "\n",
    "print(\"Synthetic out:\", OUTPUT_ROOT)\n",
    "print(\"Checkpoints  :\", CHECKPOINTS)\n",
    "print(\"Previews     :\", PREVIEWS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5b4321",
   "metadata": {},
   "source": [
    "\n",
    "### Optional: Install libraries (run only if needed)\n",
    "If your environment is missing packages, run the cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d6d656",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
    "# !pip install pillow tqdm matplotlib imagehash\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7478fc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =======================================\n",
    "# Imports\n",
    "# =======================================\n",
    "import os, sys, math, json, random, time\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6470ec25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =======================================\n",
    "# Data utilities\n",
    "# =======================================\n",
    "def find_leaf_processed_dirs(root: Path):\n",
    "    root = Path(root)\n",
    "    out = []\n",
    "    for p in root.rglob(\"*\"):\n",
    "        if p.is_dir() and p.name.endswith(\"_processed\"):\n",
    "            entries = list(p.glob(\"*\"))\n",
    "            has_images = any(e.suffix.lower() in {\".png\", \".jpg\", \".jpeg\", \".bmp\", \".tif\", \".tiff\"} for e in entries if e.is_file())\n",
    "            has_subdirs = any(e.is_dir() for e in entries)\n",
    "            if has_images and not has_subdirs:\n",
    "                out.append(p)\n",
    "    return sorted(out)\n",
    "\n",
    "def class_name_from_processed_dir(d: Path):\n",
    "    name = d.name\n",
    "    if name.endswith(\"_processed\"):\n",
    "        name = name[:-10]\n",
    "    return name\n",
    "\n",
    "processed_dirs = find_leaf_processed_dirs(Path(DATASET_MEDICAL_ROOT))\n",
    "assert len(processed_dirs) > 0, \"No *_processed leaf directories found under medical/\"\n",
    "class_names = [class_name_from_processed_dir(d) for d in processed_dirs]\n",
    "num_classes = len(class_names)\n",
    "\n",
    "print(f\"Found {num_classes} classes:\")\n",
    "for d, cname in zip(processed_dirs, class_names):\n",
    "    print(\" -\", cname, \"from\", d)\n",
    "\n",
    "class_to_idx = {c:i for i,c in enumerate(class_names)}\n",
    "idx_to_class = {i:c for c,i in class_to_idx.items()}\n",
    "\n",
    "class ProcessedImagesDataset(Dataset):\n",
    "    def __init__(self, processed_dirs, class_to_idx, image_size=256, channels=3, augment=True):\n",
    "        self.samples = []\n",
    "        self.class_to_idx = class_to_idx\n",
    "        self.channels = channels\n",
    "\n",
    "        for d in processed_dirs:\n",
    "            cname = d.name[:-10] if d.name.endswith(\"_processed\") else d.name\n",
    "            y = class_to_idx[cname]\n",
    "            for imgp in d.glob(\"*.png\"):\n",
    "                self.samples.append((imgp, y))\n",
    "\n",
    "        aug_list = []\n",
    "        if augment:\n",
    "            aug_list += [\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomRotation(degrees=5),\n",
    "            ]\n",
    "        aug_list += [\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5]*channels, [0.5]*channels),\n",
    "        ]\n",
    "        self.transform = transforms.Compose(aug_list)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        p, y = self.samples[idx]\n",
    "        img = Image.open(p).convert(\"RGB\")\n",
    "        x = self.transform(img)\n",
    "        return x, y\n",
    "\n",
    "dataset = ProcessedImagesDataset(processed_dirs, class_to_idx, image_size=image_size, channels=channels, augment=True)\n",
    "print(\"Dataset size:\", len(dataset))\n",
    "counts = Counter([y for _, y in dataset.samples])\n",
    "for k,v in counts.items():\n",
    "    print(f\"{idx_to_class[k]}: {v}\")\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b78da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =======================================\n",
    "# ACGAN models\n",
    "# =======================================\n",
    "class GenBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_ch, out_ch, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "class DisBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, bn=True):\n",
    "        super().__init__()\n",
    "        layers = [nn.Conv2d(in_ch, out_ch, 4, 2, 1, bias=False)]\n",
    "        if bn:\n",
    "            layers.append(nn.BatchNorm2d(out_ch))\n",
    "        layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, num_classes, img_channels=3, base=64):\n",
    "        super().__init__()\n",
    "        cond_dim = num_classes\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(latent_dim + cond_dim, base*16*4*4),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.net = nn.Sequential(\n",
    "            GenBlock(base*16, base*8),  # 8x8\n",
    "            GenBlock(base*8, base*4),   # 16x16\n",
    "            GenBlock(base*4, base*2),   # 32x32\n",
    "            GenBlock(base*2, base),     # 64x64\n",
    "            GenBlock(base, base//2),    # 128x128\n",
    "            nn.ConvTranspose2d(base//2, img_channels, 4, 2, 1, bias=False),  # 256x256\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, z, y):\n",
    "        y_onehot = F.one_hot(y, num_classes=self.num_classes).float()\n",
    "        zc = torch.cat([z, y_onehot], dim=1)\n",
    "        x = self.fc(zc).view(z.size(0), -1, 4, 4)\n",
    "        img = self.net(x)\n",
    "        return img\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, num_classes, img_channels=3, base=64):\n",
    "        super().__init__()\n",
    "        self.feature = nn.Sequential(\n",
    "            DisBlock(img_channels, base, bn=False),\n",
    "            DisBlock(base, base*2),\n",
    "            DisBlock(base*2, base*4),\n",
    "            DisBlock(base*4, base*8),\n",
    "            DisBlock(base*8, base*16),\n",
    "            nn.Conv2d(base*16, base*16, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "        self.adv_head = nn.Conv2d(base*16, 1, 4, 1, 0, bias=False)\n",
    "        self.cls_head = nn.Conv2d(base*16, num_classes, 4, 1, 0, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        f = self.feature(x)\n",
    "        adv = self.adv_head(f).view(x.size(0), 1)\n",
    "        cls = self.cls_head(f).view(x.size(0), -1)\n",
    "        return adv, cls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c852a51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =======================================\n",
    "# Training loop with AMP\n",
    "# =======================================\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "G = Generator(latent_dim, num_classes, img_channels=channels).to(device)\n",
    "D = Discriminator(num_classes, img_channels=channels).to(device)\n",
    "\n",
    "optG = torch.optim.Adam(G.parameters(), lr=lr_G, betas=(beta1, beta2))\n",
    "optD = torch.optim.Adam(D.parameters(), lr=lr_D, betas=(beta1, beta2))\n",
    "\n",
    "adv_loss = nn.BCEWithLogitsLoss().to(device)\n",
    "cls_loss = nn.CrossEntropyLoss().to(device)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "CHECKPOINTS.mkdir(parents=True, exist_ok=True)\n",
    "PREVIEWS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "fixed_z = torch.randn(samples_per_grid, latent_dim, device=device)\n",
    "fixed_labels = torch.tensor([i % num_classes for i in range(samples_per_grid)], device=device)\n",
    "\n",
    "def preview(epoch):\n",
    "    G.eval()\n",
    "    with torch.no_grad():\n",
    "        imgs = G(fixed_z, fixed_labels)\n",
    "        imgs = (imgs + 1) * 0.5\n",
    "        grid = make_grid(imgs.clamp(0,1), nrow=int(math.sqrt(samples_per_grid)))\n",
    "        save_image(grid, PREVIEWS / f\"epoch_{epoch:04d}.png\")\n",
    "    G.train()\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    pbar = tqdm(loader, desc=f\"Epoch {epoch}/{num_epochs}\", leave=False)\n",
    "    for real, y in pbar:\n",
    "        real = real.to(device, non_blocking=True)\n",
    "        y    = y.to(device, non_blocking=True)\n",
    "        bsz  = real.size(0)\n",
    "\n",
    "        # ---- Train D ----\n",
    "        z = torch.randn(bsz, latent_dim, device=device)\n",
    "        with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
    "            fake = G(z, y).detach()\n",
    "            adv_real, cls_real = D(real)\n",
    "            d_real = adv_loss(adv_real, torch.ones_like(adv_real))\n",
    "            c_real = cls_loss(cls_real, y)\n",
    "\n",
    "            adv_fake, _ = D(fake)\n",
    "            d_fake = adv_loss(adv_fake, torch.zeros_like(adv_fake))\n",
    "\n",
    "            d_loss = d_real + d_fake + c_real\n",
    "\n",
    "        optD.zero_grad(set_to_none=True)\n",
    "        scaler.scale(d_loss).backward()\n",
    "        scaler.step(optD)\n",
    "\n",
    "        # ---- Train G ----\n",
    "        z = torch.randn(bsz, latent_dim, device=device)\n",
    "        y_fake = torch.randint(0, num_classes, (bsz,), device=device)\n",
    "        with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
    "            gen = G(z, y_fake)\n",
    "            adv_gen, cls_gen = D(gen)\n",
    "            g_adv = adv_loss(adv_gen, torch.ones_like(adv_gen))\n",
    "            g_cls = cls_loss(cls_gen, y_fake)\n",
    "            g_loss = g_adv + g_cls\n",
    "\n",
    "        optG.zero_grad(set_to_none=True)\n",
    "        scaler.scale(g_loss).backward()\n",
    "        scaler.step(optG)\n",
    "        scaler.update()\n",
    "\n",
    "        pbar.set_postfix(d=float(d_loss.item()), g=float(g_loss.item()))\n",
    "\n",
    "    if epoch % save_preview_every == 0 or epoch == 1:\n",
    "        preview(epoch)\n",
    "        torch.save({\n",
    "            \"G\": G.state_dict(),\n",
    "            \"D\": D.state_dict(),\n",
    "            \"epoch\": epoch,\n",
    "            \"class_to_idx\": class_to_idx,\n",
    "            \"config\": {\n",
    "                \"latent_dim\": latent_dim, \"image_size\": image_size,\n",
    "                \"channels\": channels, \"num_classes\": num_classes\n",
    "            }\n",
    "        }, CHECKPOINTS / f\"acgan_epoch_{epoch:04d}.pt\")\n",
    "\n",
    "print(\"Training complete.\")\n",
    "preview(num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98b397e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =======================================\n",
    "# Synthetic generation to balance classes\n",
    "# =======================================\n",
    "OUTPUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "from collections import Counter\n",
    "class_counts = Counter([y for _, y in dataset.samples])\n",
    "max_count = max(class_counts.values())\n",
    "\n",
    "def generate_for_class(cls_idx, n_images, batch=64):\n",
    "    out_dir = OUTPUT_ROOT / idx_to_class[cls_idx]\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    G.eval()\n",
    "    saved = 0\n",
    "    with torch.no_grad():\n",
    "        while saved < n_images:\n",
    "            cur = min(batch, n_images - saved)\n",
    "            z = torch.randn(cur, latent_dim, device=device)\n",
    "            y = torch.full((cur,), cls_idx, device=device, dtype=torch.long)\n",
    "            imgs = G(z, y)\n",
    "            imgs = (imgs + 1) * 0.5\n",
    "            for i in range(cur):\n",
    "                save_image(imgs[i].clamp(0,1), out_dir / f\"gen_{saved+i:06d}.png\")\n",
    "            saved += cur\n",
    "    return saved\n",
    "\n",
    "gen_plan = {}\n",
    "for k,v in class_counts.items():\n",
    "    target = max_count if balance_to_largest else v\n",
    "    need = max(0, target - v)\n",
    "    gen_plan[k] = need\n",
    "\n",
    "print(\"Generation plan (#images to create per class):\")\n",
    "for k,need in gen_plan.items():\n",
    "    print(f\" - {idx_to_class[k]}: need {need}\")\n",
    "\n",
    "total_to_gen = sum(gen_plan.values())\n",
    "print(\"Total to generate:\", total_to_gen)\n",
    "\n",
    "if total_to_gen > 0:\n",
    "    for k,need in gen_plan.items():\n",
    "        if need > 0:\n",
    "            made = generate_for_class(k, need, batch=64)\n",
    "            print(f\"Generated {made} images for class {idx_to_class[k]}\")\n",
    "else:\n",
    "    print(\"Dataset already balanced by max class count; no generation needed.\")\n",
    "\n",
    "print(\"Synthetic images saved under:\", OUTPUT_ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f5004e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =======================================\n",
    "# Quick visualization\n",
    "# =======================================\n",
    "from IPython.display import display\n",
    "preview_imgs = sorted(PREVIEWS.glob(\"*.png\"))\n",
    "if preview_imgs:\n",
    "    display(Image.open(preview_imgs[-1]).resize((512,512)))\n",
    "else:\n",
    "    print(\"No preview images found yet.\")\n",
    "\n",
    "for c in list(idx_to_class.values())[:3]:\n",
    "    sample_dir = OUTPUT_ROOT / c\n",
    "    if sample_dir.exists():\n",
    "        samples = list(sample_dir.glob(\"*.png\"))[:9]\n",
    "        if samples:\n",
    "            grid = []\n",
    "            for p in samples:\n",
    "                grid.append(transforms.ToTensor()(Image.open(p).convert(\"RGB\")))\n",
    "            grid = torch.stack(grid, dim=0)\n",
    "            save_image(make_grid(grid, nrow=3), PREVIEWS / f\"synthetic_{c}.png\")\n",
    "            display(Image.open(PREVIEWS / f\"synthetic_{c}.png\").resize((512,512)))\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
